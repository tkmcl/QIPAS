{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quntitative DHI analysis\n",
    "# written by Y.Fan May/2018\n",
    "# Last update on June 4/2018\n",
    "'''\n",
    "This is a module to analyze the seismic DHI features. There are five main features measured in this module: up down ratio, \n",
    "AVO, contact sharpness, contact comformability to structure and pachiness of DHI. These features defines main characters of \n",
    "the hydrocarbon related DHI in Brunei. For a new basin or field, depending on the rock and fluid properties, different \n",
    "features of DHI may be more suitable to differenciate hydrocarbons. The selection of DHI features can be adviced by the \n",
    "corresponding QI team. Besides the five main features, we also measure fluid contacts (CTD) and potential hydrocarbon volume.\n",
    "'''\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "def DHI_analysis(dep_map,amp_full,amp_far,amp_near,polygon_file,fajar_property_database,well_top):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dep_map: file name of the depth map in ascii format. e.g. exported ascii map from NDI.\n",
    "    amp_full: file name of the full amplitude in ascii format. Same as above. Files has three columns x,y,z(or amp)\n",
    "    amp_far: file name of the far amplitude in ascii format.\n",
    "    amp_near: file name of the near amplitude in ascii format.\n",
    "    polygon_file: file of the polygon file in ascii format that is exported from NDI. The polygon file can have multiple \n",
    "              polygons from the same corresponding map  \n",
    "    fajar_property_database: file of the FAJAR property database\n",
    "    well_top: well top marker of the corresponding map. This marker is used to extract property from FAJAR database. \n",
    "              e.g. 'CM1', 'K10.10'\n",
    "    '''\n",
    "    #################################################################################\n",
    "    ''''## step1: load data'''\n",
    "    T0 = time.time()  # start clock to calculate program running time\n",
    "    xd,yd,dep = np.loadtxt(f_dep, skiprows=1, unpack=True) # load depth file\n",
    "    print('depth map loaded. loading amplitude full ...')\n",
    "    xa,ya,amp = np.loadtxt(f_amp, skiprows=1, unpack=True) # load amplitude file\n",
    "    print('amplitude full loaded. loading amplitude far ...')\n",
    "    xf,yf,ampf = np.loadtxt(f_amp_far, skiprows=1, unpack=True) # load amplitude file\n",
    "    print('amplitude far loaded. loading amplitude near ...')\n",
    "    xn,yn,ampn = np.loadtxt(f_amp_near, skiprows=1, unpack=True) # load amplitude file\n",
    "    print('all maps loaded. loading FAJAR database...')\n",
    "    fajar = pd.read_csv(fajar_property_database, header=0)\n",
    "    top=fajar.Pick_Name==well_top  # pick the welltop to work on \n",
    "    print('FAJAR database loaded.')\n",
    "    dt1 = time.time()-T0; \n",
    "    print('total data loading time',round(dt1),'s.')\n",
    "    \n",
    "    ## load and format polygons\n",
    "    t0=time.time()\n",
    "\n",
    "    # define data structures for multiple blocks\n",
    "    poly_all=[]\n",
    "    df_blk = pd.read_table(polygon_file,delim_whitespace=True,header=None)\n",
    "    df_blk.columns = ['x', 'y','z','f1','f2','f3','poly_name']\n",
    "    plist = df_blk.poly_name.unique()  # polygon name list\n",
    "\n",
    "    for pdx in range(0,len(plist)):\n",
    "        df_blk0 = df_blk[df_blk.poly_name==plist[pdx]]\n",
    "        df_blk0.index = pd.RangeIndex(len(df_blk0.index))\n",
    "        poly_blk = make_poly1(df_blk0)\n",
    "        poly_all.append(poly_blk)\n",
    "    dt2 = time.time()-t0;\n",
    "    print('Converting polygon to Python format. this step takes',round(dt2,2),'s')\n",
    "    \n",
    "    #################################################################################\n",
    "    '''## Step2: Dividing the maps into each block polygons'''\n",
    "    amp_blk_all = [];dep_blk_all = [];x_blk_all = [];y_blk_all=[];\n",
    "    ampf_blk_all = [];xf_blk_all=[];yf_blk_all=[];\n",
    "    ampn_blk_all = [];xn_blk_all=[];yn_blk_all=[];\n",
    "\n",
    "    ## cut the amplitude with blk polygon\n",
    "    time0=time.time()\n",
    "    for pdx in range(0,len(poly_all)):\n",
    "        poly = poly_all[pdx]\n",
    "        amp_blk,xa_blk,ya_blk=cut_map_poly1(amp,xa,ya,poly)\n",
    "        dep_blk,xd_blk,yd_blk=cut_map_poly1(dep,xd,yd,poly)\n",
    "        points_a = np.asarray(list(map(lambda x,y:[x,y],xa_blk,ya_blk)))\n",
    "        points_d = np.asarray(list(map(lambda x,y:[x,y],xd_blk,yd_blk)))\n",
    "        sampling = 20 # spatial sampling in xy for new grid \n",
    "        nx = int((max(xa_blk)-min(xa_blk))/sampling)\n",
    "        ny = int((max(ya_blk)-min(ya_blk))/sampling)\n",
    "        grid_x, grid_y = np.mgrid[min(xa_blk):max(xa_blk):nx*1j, min(ya_blk):max(ya_blk):ny*1j] # new grid\n",
    "        grid_a = griddata(points_a, np.asarray(amp_blk), (grid_x, grid_y), method='cubic')  # interp to new grid\n",
    "        grid_d = griddata(points_d, np.asarray(dep_blk), (grid_x, grid_y), method='cubic')  # interp to new grid\n",
    "        amp_blk,x_blk,y_blk = cut_map_poly1_grid(grid_a,grid_x, grid_y,poly)\n",
    "        dep_blk,x_blk,y_blk = cut_map_poly1_grid(grid_d,grid_x, grid_y,poly)\n",
    "        amp_blk_all.append(amp_blk);dep_blk_all.append(dep_blk);\n",
    "        x_blk_all.append(x_blk);y_blk_all.append(y_blk);\n",
    "    time1 = time.time()\n",
    "    dt31 = round(time1-time0,1)\n",
    "    print('full stack map dividing finished. it takes',dt31,'s.')\n",
    "\n",
    "    # far\n",
    "    for pdx in range(0,len(poly_all)):\n",
    "        poly = poly_all[pdx]\n",
    "        ampf_blk,xf_blk,yf_blk=cut_map_poly1(ampf,xf,yf,poly)\n",
    "        ampf_blk_all.append(ampf_blk);\n",
    "        xf_blk_all.append(xf_blk);yf_blk_all.append(yf_blk);\n",
    "    time2 = time.time()\n",
    "    dt32 = round(time2-time1,1)\n",
    "    print('far stack map dividing finished. it takes',dt32,'s.')\n",
    "\n",
    "    # near\n",
    "    for pdx in range(0,len(poly_all)):\n",
    "        poly = poly_all[pdx]\n",
    "        ampn_blk,xn_blk,yn_blk=cut_map_poly1(ampn,xn,yn,poly)\n",
    "        ampn_blk_all.append(ampn_blk);\n",
    "        xn_blk_all.append(xn_blk);yn_blk_all.append(yn_blk);\n",
    "    time3 = time.time()\n",
    "    dt33 = round(time3-time2,1)\n",
    "    print('near stack map dividing finished. it takes',dt33,'s.')    \n",
    "\n",
    "    #################################################################################\n",
    "    '''## Step3: DHI analysis in each block.'''\n",
    "    dataout = [];\n",
    "    #for pdx in range(0,len(plist)):\n",
    "    ply = 7        # to QC a particular block\n",
    "    for pdx in range(ply-1,ply):\n",
    "        t0 = time.time() # loop starting time\n",
    "        \"\"\"\n",
    "        choose the block polygon\n",
    "        \"\"\"\n",
    "        poly_blk = poly_all[pdx]\n",
    "        amp_blk = np.asarray(amp_blk_all[pdx]);dep_blk = np.asarray(dep_blk_all[pdx]);\n",
    "        x_blk = np.asarray(x_blk_all[pdx]);y_blk=np.asarray(y_blk_all[pdx]);\n",
    "        ampf_blk = np.asarray(ampf_blk_all[pdx]);\n",
    "        xf_blk = np.asarray(xf_blk_all[pdx]);yf_blk=np.asarray(yf_blk_all[pdx]);    \n",
    "        ampn_blk = np.asarray(ampn_blk_all[pdx]);\n",
    "        xn_blk = np.asarray(xn_blk_all[pdx]);yn_blk=np.asarray(yn_blk_all[pdx]);\n",
    "        print(len(ampf_blk),len(xf_blk))\n",
    "\n",
    "        \"\"\"\n",
    "        CTD stacking in polygon\n",
    "        \"\"\"\n",
    "        # convert grid data dep and amp to 1d array\n",
    "        dep_blk_grid = dep_blk.copy();x_blk_grid=x_blk.copy();y_blk_grid=y_blk.copy();\n",
    "        dep_blk1 = dep_blk.flatten(); amp_blk1 = amp_blk.flatten()\n",
    "        x_blk1 = x_blk.flatten(); y_blk1 = y_blk.flatten()\n",
    "        dep_blk = dep_blk1[dep_blk1>0];amp_blk = amp_blk1[dep_blk1>0]\n",
    "        x_blk = x_blk1[dep_blk1>0];y_blk = y_blk1[dep_blk1>0]\n",
    "\n",
    "        win_half=15; #half window size in meter for moving average\n",
    "        # define a regular grid to do moving average\n",
    "        dep_reg_blk = np.linspace(np.min(dep_blk), np.max(dep_blk), num=np.int(np.max(dep_blk)-np.min(dep_blk))) # basically every meter.\n",
    "        amp_reg_blk = running_mean(amp_blk,dep_blk,dep_reg_blk,win_half)\n",
    "\n",
    "        ## calculate the derivative of the CTD: the slope of the amplitude change\n",
    "        dx = dep_reg_blk[1]-dep_reg_blk[0]\n",
    "        dydx = np.gradient(amp_reg_blk, dx)\n",
    "        dydx2 = np.gradient(dydx,dx)\n",
    "        dydx_avg = running_mean(dydx,dep_reg_blk,dep_reg_blk,win_half)\n",
    "        win_c = 300;  # the half window to search for contact\n",
    "\n",
    "        d_max = dep_reg_blk[amp_reg_blk==np.max(amp_reg_blk[0:int(0.8*len(amp_reg_blk))])]  # depth of the brightest amplitude\n",
    "        d_c = dep_reg_blk[dydx_avg==np.min(dydx_avg[(dep_reg_blk>d_max-win_c/8)&(dep_reg_blk<d_max+win_c)])]\n",
    "\n",
    "        \"\"\"\n",
    "        updip downdip\n",
    "        \"\"\"\n",
    "        # create up down polygon \n",
    "        poly_up,poly_down,buffer = cut_poly(x_blk,y_blk,dep_blk,poly_blk,d_c)\n",
    "    #    buffer = 20; # buffer gap for up down \n",
    "        amp_ud = amp_blk[dep_blk<d_c]; dep_ud = dep_blk[dep_blk<d_c];\n",
    "        amp_dd = amp_blk[dep_blk>d_c+buffer]; dep_dd = dep_blk[dep_blk>d_c+buffer];\n",
    "        # add a 'best fit' normal distribution line to the histogram\n",
    "        mu_ud = np.mean(amp_ud); sd_ud=np.std(amp_ud);\n",
    "        mu_dd = np.mean(amp_dd); sd_dd=np.std(amp_dd);\n",
    "        ud_ratio = mu_ud/mu_dd\n",
    "\n",
    "        \"\"\"\n",
    "        contact sharpness\n",
    "        \"\"\"\n",
    "        idx_win = 10; # the half window around contact to measure sharpness \n",
    "        idx_c= np.where(dep_reg_blk==d_c)\n",
    "        amp_ratio_c = amp_reg_blk[idx_c[0]-idx_win]/amp_reg_blk[idx_c[0]+idx_win]\n",
    "        # sharpness here is the amplitude ratio above and below contact comparing with updip downdip ratio\n",
    "        # this can be affected by the variation of amplitude updip and downdip; \n",
    "        sharpness = amp_ratio_c[0]/ud_ratio  \n",
    "        # sharpness1 is the percentage change in amplitude around contact +-idx_win. it is a more direct measurement \n",
    "        # of how sharp the contact is. \n",
    "        sharpness1 = (amp_reg_blk[idx_c[0]-idx_win]-amp_reg_blk[idx_c[0]+idx_win])/amp_reg_blk[idx_c[0]]\n",
    "        sharpness = sharpness1[0]\n",
    "\n",
    "        \"\"\"\n",
    "        contact comformability\n",
    "        \"\"\"\n",
    "        win_c = 5  # half windown 5m around contact\n",
    "        amp_c = amp_blk[(dep_blk<d_c+win_c)&(dep_blk>d_c-win_c)]  # find amplitude dc within the window\n",
    "        mu_amp_c = np.mean(amp_c); sd_amp_c=np.std(amp_c);\n",
    "        # normalize standard deviation by the amplitude itself\n",
    "        sd_amp_c_norm = round(sd_amp_c/mu_amp_c,3);\n",
    "\n",
    "        \"\"\"\n",
    "        AVO: far near ratio\n",
    "        \"\"\"\n",
    "        ## Updip\n",
    "        ampf_ud = [];xf_ud=[];yf_ud=[];\n",
    "        ampn_ud = [];xn_ud=[];yn_ud=[];\n",
    "        ampf_dd = [];xf_dd=[];yf_dd=[];\n",
    "        ampn_dd = [];xn_dd=[];yn_dd=[];\n",
    "\n",
    "        # far stack   \n",
    "        ampf_ud,xf_ud,yf_ud = cut_map_poly1(ampf_blk,xf_blk,yf_blk,poly_up)\n",
    "        ampf_dd,xf_dd,yf_dd = cut_map_poly1(ampf_blk,xf_blk,yf_blk,poly_down)\n",
    "\n",
    "        # near stack\n",
    "        ampn_ud,xn_ud,yn_ud = cut_map_poly1(ampn_blk,xn_blk,yn_blk,poly_up)\n",
    "        ampn_dd,xn_dd,yn_dd = cut_map_poly1(ampn_blk,xn_blk,yn_blk,poly_down)\n",
    "\n",
    "        # up down ratio (far and near) and AVO \n",
    "        udf_ratio = np.mean(ampf_ud)/np.mean(ampf_dd);\n",
    "        udn_ratio = np.mean(ampn_ud)/np.mean(ampn_dd);\n",
    "        fn_ratio_up = np.mean(ampf_ud)/np.mean(ampn_ud);\n",
    "        fn_ratio_down = np.mean(ampf_dd)/np.mean(ampn_dd);\n",
    "\n",
    "        \"\"\"\n",
    "        calculate the size of updip area\n",
    "        \"\"\"\n",
    "        area_up = round(PolygonArea(poly_up)/10**6,3)  # in km square\n",
    "        area_blk = round(PolygonArea(poly_blk)/10**6,3)  # in km square\n",
    "        # dip of structure\n",
    "        win_c1 = 1  # half windown 1m around contact\n",
    "        x_blk_c = x_blk[(dep_blk<d_c+win_c1)&(dep_blk>d_c-win_c1)]  # find amplitude dc within the window\n",
    "        y_blk_c = y_blk[(dep_blk<d_c+win_c1)&(dep_blk>d_c-win_c1)]\n",
    "        x0 = x_blk[dep_blk==np.min(dep_blk)]; y0 = y_blk[dep_blk==np.min(dep_blk)]; # the shallowest point in the structure\n",
    "        dist0 = np.sqrt((x0-x_blk_c)**2+(y0-y_blk_c)**2)   # distance between the crest to the points along contact\n",
    "        theta = np.arctan( (d_c-np.min(dep_blk))/np.min(dist0) )  # the dip angle\n",
    "        theta1 = round(57.2958*theta[0],1)\n",
    "        area_up1 = round(area_up/np.cos(theta[0]),2)  # correct area size in 3D\n",
    "\n",
    "        \"\"\"\n",
    "        Estimate the volume using the information from FAJAR database \n",
    "        \"\"\"\n",
    "        fajar1 = fajar[top].reset_index(drop=True)\n",
    "        w_x = fajar1['Easting']; w_y = fajar1['Northing'];w_name=fajar1['Wellbore_Name']\n",
    "\n",
    "        # select the well tops in the updip polygon\n",
    "        wx_blk=[];wy_blk=[];color_top=[];\n",
    "        sh2_thrd = 0.3; # saturation threshhold to defind HC \n",
    "        for idx in range(0,len(w_x)):\n",
    "            if inside_polygon(w_x[idx], w_y[idx], poly_up):\n",
    "                print(w_name[idx])\n",
    "                wx_blk.append(w_x[idx]);wy_blk.append(w_y[idx]);\n",
    "\n",
    "        Call = 'TBD'   # default fluid is unknow\n",
    "        if len(wx_blk)>0:     # if there are wells updip \n",
    "            # select the dataframe for the wells in the updip polygon\n",
    "            fajar2=pd.DataFrame(columns=fajar1.columns)\n",
    "            for idx in range(0,len(wx_blk)):\n",
    "                fajar2=fajar2.append(fajar1[(w_y==wy_blk[idx])&(w_x==wx_blk[idx])])\n",
    "            # define properties\n",
    "            fajar2=fajar2.reset_index(drop=True)\n",
    "            sh2 = np.max(fajar2.SH2mean);print(sh2)\n",
    "            por2 = np.mean(fajar2.POR2mean)\n",
    "            d_net = np.mean(fajar2[fajar2.NET_THICKNESS_SUM_TVDSS_DIFF>0].NET_THICKNESS_SUM_TVDSS_DIFF)\n",
    "            color_top=fajar2.SH2mean.copy()\n",
    "            color_top[color_top>=sh2_thrd]=2        # define HC color\n",
    "            color_top[color_top<sh2_thrd]=0         # Brine color\n",
    "            color_top[np.isnan(color_top)]=1        # unknow saturation\n",
    "            if any(fajar2[fajar2.SH2mean>0].SH2mean>=sh2_thrd):\n",
    "                Call = 'HC'\n",
    "            else:#all(fajar2[fajar2.SH2mean>0].SH2mean<sh2_thrd):\n",
    "                Call = 'Brine'\n",
    "        else:\n",
    "            p = np.asarray(poly_up)\n",
    "            cent=((np.max(p[:,0])+np.min(p[:,0]))/2,(np.max(p[:,1])+np.min(p[:,1]))/2)   # center of the polygon\n",
    "            dist = np.sqrt((cent[0]-w_x)**2+(cent[1]-w_y)**2) \n",
    "            fajar2 = fajar1[dist==np.min(dist)]\n",
    "            por2 = np.mean(fajar2.POR2mean)\n",
    "            d_net = np.mean(fajar2[fajar2.NET_THICKNESS_SUM_TVDSS_DIFF>0].NET_THICKNESS_SUM_TVDSS_DIFF) \n",
    "            sh2 = np.mean(fajar1[fajar1.SH2mean>sh2_thrd].SH2mean)    # estimate HC saturation from the area\n",
    "\n",
    "        # estimated the HC volumes\n",
    "        HC_volume = area_up1*d_net*por2*sh2*10**6   # in cubic meters\n",
    "\n",
    "        \"\"\"\n",
    "        output\n",
    "        \"\"\"\n",
    "        name =  plist[pdx]\n",
    "        #define dataframe\n",
    "        #data = [{'a': name, 'b': round(ud_ratio,2), 'c': round(udf_ratio,2),'d': round(udn_ratio,2),'e': round(fn_ratio_up,2),'f': round(fn_ratio_down,2), 'g':round(sharpness,2),'h':round(sd_ud/mu_ud,2),'i':round(d_c[0])-10,'j':sd_amp_c_norm,'k':theta1,'l':area_up1,'m':'TBD'}]\n",
    "        #data1 = [{'a': name, 'b': round(ud_ratio,2), 'e': round(fn_ratio_up,2), 'g':round(sharpness,2), 'h':round(sd_ud/mu_ud,2),'j':sd_amp_c_norm,'i':round(d_c[0])-10,'l':area_up1,'m':'TBD'}]\n",
    "        data1 = [{'a': name, 'b': round(ud_ratio,2), 'c': round(fn_ratio_up,2), 'd':round(sharpness,2),'e':round(sd_ud/mu_ud,2),'f':sd_amp_c_norm,'g':round(d_c[0])-10,'h':area_up1,'i':HC_volume,'j':Call}]\n",
    "        dataout = dataout+data1\n",
    "        dt = round(time.time()-t0,3)\n",
    "        print(name,':',len(amp_blk), 'data points; running time',dt,'s')\n",
    "\n",
    "    #################################################################################\n",
    "    '''## Step4: Return dataframe.'''\n",
    "    df = pd.DataFrame(dataout)\n",
    "    df.columns=['block_name','up_down_ratio','farOVnear_updip','contactSharp_normalized','amp_up_Coherence_normalized','comformability','contact_CTD','area_updip','HC_Volume_estimate','Call']\n",
    "    dt = time.time()-T0; \n",
    "    print('total running time',round(dt),'s.')\n",
    "    return df \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "## define a function to check if a point is inside of a polygon\n",
    "def inside_polygon(x, y, points):\n",
    "    \"\"\"\n",
    "    Return True if a coordinate (x, y) is inside a polygon defined by\n",
    "    a list of verticies [(x1, y1), (x2, x2), ... , (xN, yN)].\n",
    "\n",
    "    Reference: http://www.ariel.com.au/a/python-point-int-poly.html\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    inside = False\n",
    "    p1x, p1y = points[0]\n",
    "    for i in range(1, n + 1):\n",
    "        p2x, p2y = points[i % n]\n",
    "        if y > min(p1y, p2y):\n",
    "            if y <= max(p1y, p2y):\n",
    "                if x <= max(p1x, p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                    if p1x == p2x or x <= xinters:\n",
    "                        inside = not inside\n",
    "        p1x, p1y = p2x, p2y\n",
    "    return inside\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# define a moving average function\n",
    "def running_mean(amp,dep,dep_reg,win_half):\n",
    "    amp_reg = [];\n",
    "    for idx in range(0,len(dep_reg)):\n",
    "        amp_reg1 = np.mean(amp[(dep<dep_reg[idx]+win_half)&(dep>dep_reg[idx]-win_half)])\n",
    "        amp_reg.append(amp_reg1)\n",
    "    amp_reg = np.asarray(amp_reg)\n",
    "    return amp_reg\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# define a function to read exported ndi polygon files and change the format for inside_polygon function\n",
    "def make_poly(f_poly_blk):\n",
    "    df_blk = pd.read_table(f_poly_blk,delim_whitespace=True, header=None)\n",
    "    df_blk.columns = ['x', 'y','z','f1','f2','f3','poly_name']\n",
    "    ## rearrange the polygon data format\n",
    "    poly_blk = [];\n",
    "    for idx in range(0,len(df_blk)):\n",
    "        point = (df_blk.x[idx],df_blk.y[idx])\n",
    "        poly_blk.append(point)\n",
    "    return poly_blk\n",
    "\n",
    "# if the polygon is already read in a dateframe\n",
    "def make_poly1(df_blk):\n",
    "    df_blk.columns = ['x', 'y','z','f1','f2','f3','poly_name']\n",
    "    ## rearrange the polygon data format\n",
    "    poly_blk = [];\n",
    "    for idx in range(0,len(df_blk)):\n",
    "        point = (df_blk.x[idx],df_blk.y[idx])\n",
    "        poly_blk.append(point)\n",
    "    return poly_blk\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# define a function to find the depth of a polygon\n",
    "def poly_dep(poly_blk,x_blk,y_blk,dep_blk):\n",
    "    d = np.zeros(len(poly_blk))\n",
    "    for idx in range(0,len(poly_blk)):\n",
    "        dist = np.sqrt( ((poly_blk[idx][0]-x_blk)**2)+((poly_blk[idx][1]-y_blk)**2) )\n",
    "        d[idx] = dep_blk[dist==np.min(dist)]\n",
    "    return d\n",
    "#-----------------------------------------------------------------------------\n",
    "# functions to cut maps into blocks\n",
    "#-----------------------------------------------------------------------------\n",
    "# cut a amplitude and depth map at the same time\n",
    "def cut_map_poly(amp0,dep0,xa0,ya0,poly):\n",
    "    amp_cut = [];dep_cut=[];x_cut=[];y_cut=[]\n",
    "    sqcut = (xa0<np.max(np.asarray(poly)[:,0]))&(xa0>np.min(np.asarray(poly)[:,0]))&(ya0<np.max(np.asarray(poly)[:,1]))&(ya0>np.min(np.asarray(poly)[:,1]))\n",
    "    amp = amp0[sqcut];dep = dep0[sqcut];xa = xa0[sqcut];ya = ya0[sqcut];\n",
    "    for idx in range(0,len(amp)):\n",
    "        if inside_polygon(xa[idx], ya[idx], poly):\n",
    "            amp_cut.append(amp[idx]);dep_cut.append(dep[idx]);\n",
    "            x_cut.append(xa[idx]);y_cut.append(ya[idx]); \n",
    "    return (amp_cut,dep_cut,x_cut,y_cut)\n",
    "# cut only the amplitude map; e.g. there is no depth map for far and near\n",
    "def cut_map_poly1(amp0,xa0,ya0,poly):\n",
    "    amp_cut = [];x_cut=[];y_cut=[]\n",
    "    sqcut = (xa0<np.max(np.asarray(poly)[:,0]))&(xa0>np.min(np.asarray(poly)[:,0]))&(ya0<np.max(np.asarray(poly)[:,1]))&(ya0>np.min(np.asarray(poly)[:,1]))\n",
    "    amp = amp0[sqcut];xa = xa0[sqcut];ya = ya0[sqcut];\n",
    "    for idx in range(0,len(amp)):\n",
    "        if inside_polygon(xa[idx], ya[idx], poly):\n",
    "            amp_cut.append(amp[idx]);\n",
    "            x_cut.append(xa[idx]);y_cut.append(ya[idx]); \n",
    "    return (amp_cut,x_cut,y_cut)\n",
    "# cut a grided map\n",
    "def cut_map_poly1_grid(grid_z,grid_x, grid_y,poly):\n",
    "    amp_cut = [];x_cut=[];y_cut=[]\n",
    "    xmin = np.min(np.asarray(poly)[:,0])\n",
    "    xmax=np.max(np.asarray(poly)[:,0]);\n",
    "    ymin = np.min(np.asarray(poly)[:,1])\n",
    "    ymax=np.max(np.asarray(poly)[:,1]);\n",
    "    \n",
    "    grid_x1 = grid_x[(grid_y[:,0]>ymin)&(grid_y[:,0]<ymax),:]\n",
    "    grid_y1 = grid_y[(grid_y[:,0]>ymin)&(grid_y[:,0]<ymax),:]\n",
    "    grid_z1 = grid_z[(grid_y[:,0]>ymin)&(grid_y[:,0]<ymax),:]\n",
    "    \n",
    "    grid_x2 = grid_x1[:,(grid_x1[0,:]>xmin)&(grid_x1[0,:]<xmax)]\n",
    "    grid_y2 = grid_y1[:,(grid_x1[0,:]>xmin)&(grid_x1[0,:]<xmax)]\n",
    "    grid_z2 = grid_z1[:,(grid_x1[0,:]>xmin)&(grid_x1[0,:]<xmax)]\n",
    "    \n",
    "    for idx in range(0,len(grid_x2[0,:])):\n",
    "        for jdx in range(0,len(grid_x2[:,0])):\n",
    "            if inside_polygon(grid_x2[jdx,idx], grid_y2[jdx,idx], poly):\n",
    "                pass\n",
    "            else:\n",
    "                grid_z2[jdx,idx]='nan'\n",
    "    return (grid_z2,grid_x2,grid_y2)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# define a function to cut the polygon based on depth\n",
    "# x,y,dep defines the structure map\n",
    "# poly_blk are the points defines the polygon\n",
    "# d_c is the estimated contact \n",
    "def cut_poly(x_blk,y_blk,dep_blk,poly_blk,d_c):\n",
    "    win_c = .5 # half depth window around d_c to search for contact\n",
    "    win_gap = 40 # depth window between up down polygon boundry\n",
    "    dc_up = d_c  # make the gap to the updip half size\n",
    "    dc_down = d_c+win_gap \n",
    "    poly_blk1 = np.asarray(poly_blk)\n",
    "    d = poly_dep(poly_blk,x_blk,y_blk,dep_blk)  # find the depth of the polygon points\n",
    "    xc_up = x_blk[(dep_blk<dc_up+win_c)&(dep_blk>dc_up-win_c)]  # find points along dc_up within the window \n",
    "    yc_up = y_blk[(dep_blk<dc_up+win_c)&(dep_blk>dc_up-win_c)]    \n",
    "    xc_down = x_blk[(dep_blk<dc_down+win_c)&(dep_blk>dc_down-win_c)]  # find points along dc_down within the window \n",
    "    yc_down = y_blk[(dep_blk<dc_down+win_c)&(dep_blk>dc_down-win_c)]\n",
    " \n",
    "    if any(item == 0 for item in [len(xc_up),len(xc_down)]): #if the gap between d_c to the end of the block is less than win_gap\n",
    "        win_gap=0\n",
    "        dc_up = d_c-win_gap\n",
    "        dc_down = d_c+win_gap\n",
    "        xc_up = x_blk[(dep_blk<dc_up+win_c)&(dep_blk>dc_up-win_c)]  # find points along dc_up within the window \n",
    "        yc_up = y_blk[(dep_blk<dc_up+win_c)&(dep_blk>dc_up-win_c)]    \n",
    "        xc_down = x_blk[(dep_blk<dc_down+win_c)&(dep_blk>dc_down-win_c)]  # find points along dc_down within the window \n",
    "        yc_down = y_blk[(dep_blk<dc_down+win_c)&(dep_blk>dc_down-win_c)]\n",
    "        \n",
    "    # updip polygon\n",
    "    poly_c_up = [];\n",
    "    for idx in range(0,len(xc_up)):\n",
    "        point = [xc_up[idx],yc_up[idx]]\n",
    "        poly_c_up.append(point)\n",
    "    poly_up = poly_blk1[d<dc_up]\n",
    "    polynew = poly_up.tolist()+poly_c_up\n",
    "    poly_up = sort_poly_point(polynew)   # sort the poly points by polar angle\n",
    "    # downdip polygon    \n",
    "    poly_c_down = [];\n",
    "    for idx in range(0,len(xc_down)):\n",
    "        point = [xc_down[idx],yc_down[idx]]\n",
    "        poly_c_down.append(point)\n",
    "    poly_down = poly_blk1[d>dc_down]\n",
    "    polynew = poly_down.tolist()+poly_c_down\n",
    "    poly_down = sort_poly_point(polynew) # sort the poly points by polar angle\n",
    "    \n",
    "    return (poly_up,poly_down,win_gap)\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# function to fort the poly points by polar angle; QC the output. If doesn't look right, try different definition of centroid. \n",
    "def sort_poly_point(poly_point):\n",
    "    # compute centroid\n",
    "    #cent=(sum([p[0] for p in poly_point])/len(poly_point),sum([p[1] for p in poly_point])/len(poly_point))\n",
    "    p = np.asarray(poly_point)\n",
    "    cent=((np.max(p[:,0])+np.min(p[:,0]))/2,(np.max(p[:,1])+np.min(p[:,1]))/2)\n",
    "    # sort by polar angle\n",
    "    poly_point.sort(key=lambda p: math.atan2(p[1]-cent[1],p[0]-cent[0]))\n",
    "    poly_point = poly_point+[poly_point[0]]\n",
    "    return poly_point\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "#calculate polygon area\n",
    "# examples\n",
    "#corners = [(2.0, 1.0), (4.0, 5.0), (7.0, 8.0)]\n",
    "def PolygonArea(corners):\n",
    "    n = len(corners) # of corners\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += corners[i][0] * corners[j][1]\n",
    "        area -= corners[j][0] * corners[i][1]\n",
    "    area = abs(area) / 2.0\n",
    "    return area\n",
    "#-----------------------------------------------------------------------------\n",
    "# \n",
    "#-----------------------------------------------------------------------------\n",
    "# import a ndi color map to python \n",
    "def import_ndi_cm(cmap_file):\n",
    "    print(cmap_file)\n",
    "    df_cm = pd.read_table(cmap_file,delim_whitespace=True,header=None,skiprows=1)\n",
    "    color=[]\n",
    "    for idx in range(0,len(df_cm)):\n",
    "        color.append((df_cm.loc[idx,0],df_cm.loc[idx,1],df_cm.loc[idx,2]))\n",
    "    \n",
    "    import matplotlib.colors as clr\n",
    "    \"\"\"    \n",
    "    fdx1 = cmap_file.rfind('/')\n",
    "    fdx2 = cmap_file.rfind('.')\n",
    "    fcmap = cmap_file[fdx1+1:fdx2]\n",
    "    fcmap_r = cmap_file[fdx1+1:fdx2]+'_r' \n",
    "    \"\"\"\n",
    "    cmap = clr.LinearSegmentedColormap.from_list('cm',color/np.max(color),len(color))\n",
    "    cmap_r = clr.LinearSegmentedColormap.from_list('cm_r',color[::-1]/np.max(color),len(color))\n",
    "    return (cmap,cmap_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7ed0097d7e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
